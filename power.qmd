# Power Calculations and Sample Size {#sec-power-calculations}

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Package imports
library(ggplot2)
```

**Statistical power** is the probability, _before a study is performed_, that a particular comparison will achieve "statistical significance" at some predetermined level.

More precisely, statistical power is the _probability that the study will not return a "Type II error", i.e. a false negative result_.

To make a power calculation, we need to make some assumptions (or, if you prefer, "propose hypotheses") about the future behaviour of data that has not yet been collected:

1. The desired **effect size** (e.g. a change in a value that is biologically meaningful, or a minimum level at which a change of value is recognised to be a change in the system)
2. The variation we expect to see in the measured outcomes (e.g. as a standard deviation)

::: {.callout-tip}
## Suppose your experiment is calculated to have 80% (0.8) power… 

The estimated power of an experiment is an incomplete description, by itself. We need also to declare the "statistical significance" threshold that has been decided on for the experiment, and the effect size we consider to be meaningful. 

If we had decided that $P=0.05$ was a suitable threshold for statistical significance, and that a change in blood glucose concentration of 2mM/L in response to administration of some drug was meaningful, we would say instead: 

**The experiment has 80% (0.8) power at $\alpha = 0.05$** (the same thing as $P = 0.05$)**, for an effect size of 2mM/L.**

But what does this mean?

Suppose that the drug works, and really does change the blood glucose concentration by 2mM/L. If you ran the experiment 50 times, with a power of 80% you would expect a statisticially significant _positive_ result (i.e. the meaningful change is detected and statistically supported) in 40 of the experiments. You would expect that in 10 of the experiments the measured change would **not** be considered to be statistically significant.
:::

::: {.callout-warning}
The choice of what makes a result significant, and what degree of statistical power is acceptable, is under researcher control, but might also be stated by a potential funder.

Typical values you might see include "80% power at 5% significance," but there is _no gold standard_ and choices should be made to suit the situation appropriately. Funding agencies typically insist that a study has at least an 80% chance of delivering a statistically significant result.
:::

## Underpowered studies

We sometimes refer to studies as being "low-powered" or "underpowered." This means that the expected effect size is small in relation to the variation (e.g. standard error) of the measurement. Low power in studies might result from having too few experimental units (e.g. individual subjects), or from measurements having relatively high _variability_ due to the effect of noisy nuisance variables.

Underpowered studies may not give a _statistically significant_ result even when a relatively large effect is found in the experiment. Worse still, because a very large effect may be required to reach statistical significance, such a result is likely to be a _statistical outlier_ and not reflect the true effect size. 

::: {.callout-important}
Even experienced scientists with many publications and long track records of research funding may never have received any formal training in experimental design or statistics. You will sometimes hear claims being made along the lines of:

> "if the effects are large enough to be seen in a small study they must be real large effects”

Unfortunately, this is not true. **"Statistically significant" results from underpowered studies are highly likely to be exaggerated, and maybe even suggest that the effect acts in the wrong direction!** (@fig-s-m-errors)
:::

```{r echo=FALSE}
#| label: fig-s-m-errors
#| fig-cap: __When effect size is small compared to the standard error of the measurement, statistical power is low, and the study is underpowered.__ In this figure, the null hypothesis is that there is no effect. The real effect size is assumed to be 2% of the size of the control, and estimates are assumed to have the same standard error as the control experiment. The curve represents the distribution of possible estimates of effect size as a Normal distribution with $\mu=2\%, \sigma=10\%$. Statistical significance is unlikely to be achieved but, if it is achieved, it is misleading, as there is a reasonable chance (39%) that the significant result will be in the wrong direction and, in any case, the magnitude of the effect size will be greatly overestimated. (Adapted from Figure 16.1 in Gelman _et al._ "Regression and Other Stories")

mu = 2   # effect
sd = 10  # std dev of null and effect
thresh = 19.6  # t-test critical value for 5% two-tailed
distn = dnorm

distn_left_limit <- function(x, mean=0, sd=1) {
    y <- distn(x, mean, sd)
    y[x > -thresh] <- NA
    return(y)
}

distn_right_limit <- function(x, mean=0, sd=1) {
    y <- distn(x, mean, sd)
    y[x < thresh] <- NA
    return(y)
}


p <- ggplot(data = data.frame(x = c(-30, 30)), aes(x)) +
  stat_function(fun = distn, n = 101, args = list(mean = mu, sd = sd)) +
  xlab("Estimated effect size") +
  ylab("") +
  scale_y_continuous(breaks = NULL) 
p = p + annotate("segment",                                          # show the null hypothesis (no effect)
                 x=0, xend=0, y=0, yend=distn(0, mu, sd) * 1.2,                       # as a dotted line
                 colour="darkorange1", linewidth=1, linetype="dashed")
p = p + annotate("text",                                             
                 x=c(0),
                 y=c(distn(0, mu, sd) * 1.25),
                 colour="darkorange3",
                 label=c("null hypothesis: 0% effect"))
p = p + annotate("segment",                                          # show the effect size
                 x=2, xend=2, y=0, yend=distn(mu, mu, sd),                       # as a solid line
                 colour="purple", linewidth=1, linetype="solid")
p = p + annotate("text",                                             
                 x=2,
                 y=0.042,
                 colour="purple",
                 label="real effect size: 2%")
p = p + stat_function(fun = distn_right_limit, n = 101, args = list(mean = mu, sd = sd),
                      geom = "area", fill = "darkolivegreen3", alpha = 0.2)
p = p + annotate("segment",                                          # show the effect size
                 x=thresh, xend=thresh, y=0, yend=distn(thresh, mu, sd),                       # as a solid line
                 colour="darkolivegreen3", linewidth=1, linetype="solid")
p = p + stat_function(fun = distn_left_limit, n = 101, args = list(mean = mu, sd = sd),
                      geom = "area", fill = "darkolivegreen3", alpha = 0.2)
p = p + annotate("text",                                             
                 x=thresh,
                 y=distn(thresh, mu, sd) * 1.5,
                 colour="darkolivegreen",
                 label="Estimate is statistically significant,\n but too large")
p = p + annotate("segment",                                          # show the effect size
                 x=-thresh, xend=-thresh, y=0, yend=distn(-thresh, mu, sd),                       # as a solid line
                 colour="darkolivegreen3", linewidth=1, linetype="solid")                 
p = p + annotate("text",                                             
                 x=-thresh,
                 y=distn(-thresh, mu, sd) * 1.7,
                 colour="darkolivegreen",
                 label="Estimate is statistically significant,\n but too large and in the wrong direction")
p = p + xlim(-40, 40) 
p = p + scale_x_continuous(breaks = c(-30, -20, -10, 0, 10, 20, 30),
                           labels = c("-30%", "-20%", "-10%", "0%", "10%", "20%", "30%")) 
p = p + theme_minimal()
p
```

::: {.callout-tip}
## Strategies to increase statistical power

There are three broad strategies available to us, as researchers, to increase the statistical power of an experiment:

1. Reduce the variability in the experiment
  - this typically means controlling for nuisance effects that cause variation in experimental units, or improving the way we measure outcome variables
2. Increase the number of experimental units
  - when we increase the number of experimental units, we _reduce the uncertainty in our estimate of error of the mean_
3. Increase the effect size
:::



## Effect size

A significant challenge for power calculation in experimental design is knowing what effect size to assume. This is usually the target of the study and not known ahead of time.

A number of strategies for choosing an effect size are possible, including:

1. trying a range of values consistent with previously-published literature
2. choose a size of effect that would be of some practical interest (e.g. we would only consider a treatment if it increased survival time by at least 20%)

::: {.callout-tip}
For reasons we will discuss below, it can be unwise to base one's estimate of effect size on the result of a single noisy study.
:::


## Power and sample size

- [NC3Rs EDA guide to sample size calculations](https://eda.nc3rs.org.uk/experimental-design-group)

Once we have settled on the structure of our experiment and made as many modifications as we can to reduce the effect of sources of variability that we can manage (such as including appropriate controls, and randomising/blocking our experimental design), one remaining factor under our control, as researchers, is the number of experimental units.

Suppose that we have decided upon an acceptable threshold for statistical significance, and an acceptable power (i.e. probability that the experiment tells us there is no meaningful effect when there is, in fact, a real meaningful effect). We may then be concerned with the cost of conducting the experiment.

::: {.callout-important}
The primary cost of an experiment involving animals is **ethical**, and measured in the potential for animal suffering. The financial cost is a secondary consideration. Funders will typically fund the experiment most likely to give robust useful results in an ethical manner, not the cheapest experiment possible.

As stated in the NC3Rs EDA guide,

> **Under-powered _in vivo_ experiments waste time and resources, lead to unnecessary animal suffering and result in erroneous biological conclusions.**

Similarly, over-powered experiments in which more animals than necessary are used to establish a result also lead to unnecessary animal suffering and are unethical.

> Ethically, when working with animals we need to conduct a harm–benefit analysis to ensure the animal use is justified for the scientific gain. Experiments should be robust, not use more or fewer animals than necessary, and truly add to the knowledge base of science. (@Karp2021-lc).
:::

We should therefore use sample size calculations to ensure that a study is neither under- nor over-powered for the stated purpose of the research.

## Practical consequences of statistical power

Considering statistical power for an experiment suggests some general approaches to improve our chances, as researchers, of seeing a statistically significant result when there is a real effect of suitable size. It also has implications about what kinds of results we are likely to find in the literature, and how we should interpret them.

### It is generally better to double the effect size than to double sample size

The standard error of estimation decreases with the square of sample size [^1]. So, if we double our sample size we should expect, approximately, a $\sqrt{2} = 1.4\times$ reduction in standard error. But if we are able to double the effect size then the actual difference we are trying to measure would be $2\times$ as large, and this will have a greater effect on the power of the experiment.

[^1]: As you will remember from [GCSE/Standard Grade mathematics](https://www.bbc.co.uk/bitesize/guides/zcjv4wx/revision/2)

::: {.callout-tip}
There are several ways to maximise effect size, including:

- setting doses as low as ethically possible in the control group, and as high as ethically possible in the treeatment group.
- choosing individuals that are especially likely to respond to the treatment
:::

Designing an experiment with the aim of increasing effect size can be difficult, and lead to other issues. When treatments are set to extreme values it can make the interpretation of the results when treatments are applied at more realistic levels doubtful. If a highly sensitive set of individuals is used, then the results may not generalise across the wider population. 

**It is for the researcher to decide and justify whether conclusive effects on a subgroup are preferred to inconclusive but more generalisable results.**

### The Winner's Curse

There is a concept in economics called the "Winner's Curse" in which the winning bidder of an auction is likely to have overpaid with respect to the value of the item being auctioned.

@fig-s-m-errors demonstrates that, by focusing on "statistically significant" results, we generate a systematically biased, overoptimistic picture of the world. In @fig-s-m-errors, we have results corresponding to a study where the true effect size could not realistically be more than 2%, but is estimated with a standard error of 10%. The curve shows the distribution of experimental estimates we would expect, shading in green those areas where the result is at least $1.96\sigma$ from the null hypothesis, i.e. "statistically significant."

::: {.callout-warning}
Using the properties of the Normal distribution we can calculate that 5.4% of the estimates will be considered to be "statistically significant", and that 39% of the time the "statistically significant" difference will be a reduction in effect - a change in the wrong direction!
:::

**We can say, without performing any experiment, that a study with these properties has essentially no chance of providing useful information.**

### Published results tend to be positive, and overestimates

Due to the effects of the "winner's curse," and well-intentioned designs that attempt to maximise effect size, it is likely that actual effects are smaller in a general population than in any specific study. 

Also, due to the systematic bias that "positive" (i.e. "statistically significant") results tend to be published when "negative" (i.e. "not significant") results are not, publication typically biases towards "statistically significant" results whose measured effect sizes, as seen from the "winner's curse," are likely to be larger than would be seen in the general population.
