# Power Calculations and Sample Size {#sec-power-calculations}

**Statistical power** is the probability, _before a study is performed_, that a particular comparison will achieve "statistical significance" at some predetermined level.

More precisely, statistical power is the _probability that the study will not return a "Type II error", i.e. a false negative result_.

To make a power calculation, we need to make some assumptions (or, if you prefer, "propose hypotheses") about the future behaviour of data that has not yet been collected:

1. The desired **effect size** (e.g. a change in a value that is biologically meaningful, or a minimum level at which a change of value is recognised to be a change in the system)
2. The variation we expect to see in the measured outcomes (e.g. as a standard deviation)

::: {.callout-tip}
## Suppose your experiment is calculated to have 80% (0.8) powerâ€¦ 

The estimated power of the experiment is incomplete by itself. We need also to declare the "statistical significance" that has been decided on for the experiment, and the effect size we consider to be meaningful. 

If we had decided that $P=0.05$ was a suitable threshold for statistical significance, and that a change in blood glucose concentration of 2mM/L in response to administration of some drug was meaningful, we would say instead: 

**The experiment has 80% (0.8) power at $\alpha = 0.05$** (the same thing as $P = 0.05$)**, for an effect size of 2mM/L.**

But what does this mean?

Suppose that the drug works, and really does change the blood glucose concentration by 2mM/L. If you ran the experiment 50 times, with a power of 80% you would expect a statisticially significant _positive_ result (i.e. the meaningful change is detected and statistically supported) in 40 of the experiments. You would expect that in 10 of the experiments the measured change would **not** be considered to be statistically significant.
:::

We sometimes informally refer to studies as being "low-powered" or "underpowered." Such studies may not give a _statistically significant_ result even when a relatively large effect is found in the experiment. Worse still, because a very large effect may be required to reach statistical significance, such a result is likely to be a _statistical outlier_ and not reflect the true effect size. Low power in studies might result from having too few experimental units (e.g. individuals), or from the measurements having relatively high _variability_.

::: {.callout-important}
## Strategies to increase statistical power

There are three broad strategies available to us, as researchers, to increase the statistical power of an experiment:

1. Reduce the variability in the experiment
  - this typically means controlling for nuisance effects that cause variation in experimental units, or improving the way we measure outcome variables
2. Increase the number of experimental units
  - when we increase the number of experimental units, we _reduce the uncertainty in our estimate of error of the mean_
3. Increase the effect size
  - 
:::

::: {.callout-warning}
The choice of what makes a result significant, and what degree of statistical power is acceptable, is under researcher control, but might also be stated by a potential funder.

Typical values you might see include "80% power at 5% significance," but there is _no gold standard_ and choices should be made to suit the situation appropriately. Funding agencies typically insist that a study has at least an 80% chance of delivering a statistically significant result.
:::

## Power and sample size

- [NC3Rs EDA guide to sample size calculations](https://eda.nc3rs.org.uk/experimental-design-group)

The relationship between power and sample size arises because there are two ways for us to improve the chance that our experiment tells us that there is some effect, if there really is some effect (i.e. to increase the power of an experiment - see above):

1. Reduce variability/noise in the experimental design
2. Increase the number of experimental units

Once we have settled on the structure of our experiment and made as many modifications as we can to reduce the amount of variability (such as including appropriate controls, and randomising/blocking our experiment), the remaining factor under our control, as researchers, is the number of experimental units.

Supposing we have decided upon an acceptable threshold for statistical significance, and an acceptable probability that the experiment tells us there is no meaningful effect when there is, in fact, a real meaningful effect. We may then be concerned with the cost of conducting the experiment.

::: {.callout-important}
The primary cost of an experiment involving animals is **ethical**, and measured in the potential for animal suffering. The financial cost is a secondary consideration.

As stated in the NC3Rs EDA guide,

> **Under-powered _in vivo_ experiments waste time and resources, lead to unnecessary animal suffering and result in erroneous biological conclusions.**

Sample size is never large enough but, similarly, over-powered experiments in which more animals than necessary are used to establish a result also lead to unnecessary animal suffering and are unethical.
:::

We should therefore use sample size calculations to ensure that a study is neither under- nor over-powered for the stated purpose of the research.

## Practical consequences of statistical power

The consideration of statistical power for an experiment leads to some general conclusions about what we should vary to improve our chances, as researchers, of seeing a statistically significant result when there is a real effect of suitable size, and also to conclusions about what kinds of results we are likely to find in the literature.

### It is better to double the effect size than to double sample size

The standard error of estimation decreases with the square of sample size [^1]. So, if we double our sample size we should expect, approximately, a $\sqrt{2} = 1.4\times$ reduction in standard error. But if we double the effect size then the actual difference we are trying to measure is $2\times$ as large.

[^1]: As you will remember from [GCSE/Standard Grade](https://www.bbc.co.uk/bitesize/guides/zcjv4wx/revision/2)

::: {.callout-tip}
There are several ways to maximise effect size, including:

- setting doses as low as ethically possible in the control group, and as high as ethically possible in the treeatment group.
- choosing individuals that are especially likely to respond to the treatment
:::

Designing an experiment with the aim of increasing effect size can be difficult. When treatments are set to extreme values it can make interpolation of the results to more realistic levels doubtful. If a highly sensitive set of individuals is used, then the results may not generalise across a wider population. 

**It is for the researcher to decide and justify whether conclusive effects on a subgroup are preferred to inconclusive but more generalisable results.**

### The winner's curse


### Published results tend to be positive, and overestimates

Due to the effects of the "winner's curse," and well-intentioned designs that attempt to maximise effect size, it is likely that actual effects are smaller in a general population than in any specific study. Also, due to the systematic bias that "positive" results tend to be published where "negative" results do not, publication typically biases towards "statistically significant" results whose measured effect sizes, as seen from the "winner's curse," are likely to be larger than would be seen in the general population.